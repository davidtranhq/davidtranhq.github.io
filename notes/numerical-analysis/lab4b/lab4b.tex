\documentclass[11pt]{article}

\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}

% Define colors for syntax highlighting
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

% Set up the MATLAB code listing style
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{mygreen},
  deletekeywords={...},
  escapeinside={\%*}{*)},
  extendedchars=true,
  frame=single,
  keepspaces=true,
  keywordstyle=\color{blue},
  language=Matlab,
  otherkeywords={*,...},
  numbers=left,
  numbersep=5pt,
  numberstyle=\tiny\color{mygray},
  rulecolor=\color{black},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stepnumber=1,
  stringstyle=\color{mymauve},
  tabsize=2,
  title=\lstname
}


% Adjust the margins if needed
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{graphicx}
\usepackage{graphicx}
\usepackage{tabto}

% Set the title and author
\title{Neural Ordinary Differential Equations}
\author{David Tran and Spencer Kelly}
\date{\today}

\begin{document}

\maketitle

\subsection*{Abstract}

In this final lab of a series of 4 labs exploring numerical methods, we discuss numerical ODE solvers. We describe Euler's method and the family of Runge-Kutta methods and the computational tradeoffs between them. In particular, we explore its usage in neural ordinary differential equations, and how different families of ODE solvers provide different tradeoffs on the approximation accuracy of the model and its computational cost.

\section{Introduction}

\section{Differential Equation Solvers}

In this section, we discuss two different methods of numerical differential equation solving, their accuracy, and their importance.

\subsection{Euler's Method}

\subsection{Runge-Kutta Methods}

\section{Neural Ordinary Differential Equations}

Neural Ordinary Differential Equations (Neural ODEs) is a deep learning approch that uses neural networks to model continuous time-series data. Unlike traditional deep learning models that rely on discrete layers and fixed architectures, Neural ODEs leverage the theory of ordinary differential equations (ODEs) to describe dynamic systems. In essence, they encapsulate the evolution of hidden states continuously over time, offering a flexible framework for modeling complex temporal dynamics.

Existing models for time-series data such as residual networks and recurrent neural networks vompose sequences of transformation to some hidden state $h$ as $h_{t + 1} = h_t + f(h_t, \theta_t)$. Crucially, these models use a fixed number of layers to represent discrete (pre-determined) time steps $t_1, \dots, t_n$. The neural ODE instead considers the sequence of transformations as a continuous and represents the transformation as an ordinary diferential equation: $\frac{dh}{dt} = f(h(t), t, \theta)$. The neural network is then trained to approximate the function $f$ as $\hat f$. The output of the model $h(t)$ at some time $t$ is computed from an initial value $h(0)$ (the input data) using any ODE solver on the ODE

$$
\frac{dh(t)}{dt} = \hat f(h(t), t, \theta)
$$

\subsection{Why machine learning for DE solving}

The advantage of machine learning for DE solving over traditional analytical or other numerical approximation methods is due to their flexibility in approximating relations of arbitrary complexity. Due to the performance of the model being a function of the amount of data available on the relation-of-interest, neural networks are particularly advantageous for solving differential equations for which the knowledge of the underlying dynamics of the relation are unknown or limited, compared to the large amount of data representing the relation.

\subsection{Application}

We use the implementation of the neural ODE described in \cite{chen2018neuralode} using the code in \cite{torchdiffeq}. We use it to learn the dynamics of a simple harmonic oscillator with slight dampening. Observe in Figure~\ref{fig:first_iteration} how the predicted trajectories and phase portrait (blue) do not match very well the ground truth (green). Although the shape of the learned vector field looks accurate, it is askew from the proper orientation of the phase portrait In Figure~\ref{fig:last_iteration}, the predicted trajectory and phase portrait nearly perfectly coincide, and we observe that the learned vector field nearly matches what one would expect from the phase portrait.


\begin{figure}
  \centering
  \includegraphics*[width=\linewidth]{000.png}
  \caption{The predicted trajectory, and the corresponding learned vector field after one iteration. The green represents the ground truth, while the blue represents the output of the model. The Dormand-Prince method, a type of Runge-Kutta ODE solver, is used to evaluate the final prediction.}
  \label{fig:first_iteration}
\end{figure}

\begin{figure}
  \centering
  \includegraphics*[width=\linewidth]{099.png}
  \caption{After 2000 iterations, using Dormand-Prince.}
  \label{fig:last_iteration}
\end{figure}

\subsubsection{Euler's Method vs Runge-Kutta}

In the above section, after learning the function $\frac{dh(t)}{dt}$, the model uses the Dormand-Prince method, an adaptive-step ODE solver that falls under the family of Runge-Kutta methods. We now compare its performance to using a fixed-point ODE solver, namely, Euler's method.

As seen in Figure~\ref{fig:first_iteration_euler}, after the first iteration, the trajectory and phase portrait match the ground-truth much worse, and the learned vector field exhibits greater distortion than when using the Dormand-Prince method as the ODE solver. Even after 2000 iterations as in Figure~\ref{fig:last_iteration_euler}, we see the model is not able to capture the trajectory as well as when using the Dormand-Prince method, as in Figure~\ref{fig:last_iteration}.

When using the Dormand-Prince method, training and evaluating the model took approximately 4 minutes and 53 seconds on an Apple M1 Pro, consisting of 8 physical cores each with 8 logical processors. Using Euler's method, training and evaluating the model took approximately 2 minutes and 3 seconds, less than half the time of the Dormand-Prince method. This demonstrates the cost-accuracy tradeoff between the two methods, and more generally, between fixed-step and adaptive-step ODE solvers.


\begin{figure}
  \centering
  \includegraphics*[width=\linewidth]{000_euler.png}
  \caption{After one iteration, using Euler's method.}
  \label{fig:first_iteration_euler}
\end{figure}

\begin{figure}
  \centering
  \includegraphics*[width=\linewidth]{099_euler.png}
  \caption{After 2000 iterations, using Euler's method.}
  \label{fig:last_iteration_euler}
\end{figure}

\section{Conclusion}

We discussed the difference between fixed-step and adaptive-step numerical ODE solvers, in particular, Euler's method and Runge-Kutta methods. We described neural ODEs, a deep learning approach that chooses to model transformations to the input data as an ordinary differential equation, allowing one to leverage the rich theory of ODE solvers to the problem of time-series modelling. We showed that, when using the Runge-Kutta methods in neural ODEs, we observe better approximations at the cost of longer compute time, and vice-versa for Euler's method.

The team was able to collaborate well and properly divide the tasks. We were able to leverage our existing experience in ODE solvers in applications for simulating fluid dynamics, as well as experience in novel machine learning research. In the future, it would be interesting to apply neural ODEs to other time-series problems with complex system dynamics, such as fluid flow simulation, weather prediction, and financial modelling.

\section{Appendix}
\begin{lstlisting}[language=Python,caption=Code for the ODE solver.]
SOLVERS = {
  'dopri8': Dopri8Solver,
  'dopri5': Dopri5Solver,
  'bosh3': Bosh3Solver,
  'fehlberg2': Fehlberg2,
  'adaptive_heun': AdaptiveHeunSolver,
  'euler': Euler,
  'midpoint': Midpoint,
  'heun3': Heun3,
  'rk4': RK4,
  'explicit_adams': AdamsBashforth,
  'implicit_adams': AdamsBashforthMoulton,
  # Backward compatibility: use the same name as before
  'fixed_adams': AdamsBashforthMoulton,
  # ~Backwards compatibility
  'scipy_solver': ScipyWrapperODESolver,
}


def odeint(func, y0, t, *, rtol=1e-7, atol=1e-9, method=None, options=None, event_fn=None):

  shapes, func, y0, t, rtol, atol, method, options, event_fn, t_is_reversed = _check_inputs(func, y0, t, rtol, atol, method, options, event_fn, SOLVERS)

  solver = SOLVERS[method](func=func, y0=y0, rtol=rtol, atol=atol, **options)

  if event_fn is None:
      solution = solver.integrate(t)
  else:
      event_t, solution = solver.integrate_until_event(t[0], event_fn)
      event_t = event_t.to(t)
      if t_is_reversed:
          event_t = -event_t

  if shapes is not None:
      solution = _flat_to_shape(solution, (len(t),), shapes)

  if event_fn is None:
      return solution
  else:
      return event_t, solution
\end{lstlisting}

\begin{lstlisting}[language=Python,caption=Code for training the network on the oscillator.]
  class ODEFunc(nn.Module):

    def __init__(self):
        super(ODEFunc, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(2, 50),
            nn.Tanh(),
            nn.Linear(50, 2),
        )

        for m in self.net.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, mean=0, std=0.1)
                nn.init.constant_(m.bias, val=0)

    def forward(self, t, y):
        return self.net(y**3)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


if __name__ == '__main__':

    ii = 0

    func = ODEFunc().to(device)
    
    optimizer = optim.RMSprop(func.parameters(), lr=1e-3)
    end = time.time()

    time_meter = RunningAverageMeter(0.97)
    
    loss_meter = RunningAverageMeter(0.97)

    for itr in range(1, args.niters + 1):
        optimizer.zero_grad()
        batch_y0, batch_t, batch_y = get_batch()
        pred_y = odeint(func, batch_y0, batch_t, method='euler').to(device)
        loss = torch.mean(torch.abs(pred_y - batch_y))
        loss.backward()
        optimizer.step()

        time_meter.update(time.time() - end)
        loss_meter.update(loss.item())

        if itr % args.test_freq == 0:
            with torch.no_grad():
                pred_y = odeint(func, true_y0, t)
                loss = torch.mean(torch.abs(pred_y - true_y))
                print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))
                visualize(true_y, pred_y, func, ii)
                ii += 1

        end = time.time()
\end{lstlisting}

\begin{thebibliography}{9}

  \bibitem{chen2018neuralode}
    Chen, R. T. Q., Rubanova, Y., Bettencourt, J., \& Duvenaud, D. (2018).
    \textit{Neural Ordinary Differential Equations}.
    Advances in Neural Information Processing Systems.
  
  \bibitem{torchdiffeq}
    Chen, R. T. Q. (2018).
    \textit{torchdiffeq}.
    Retrieved from \url{https://github.com/rtqichen/torchdiffeq}

  
  
\end{thebibliography}



  


\end{document}

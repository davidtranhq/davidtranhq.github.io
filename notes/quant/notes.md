---
toc: true
layout: default
order: 1
---

# Quant Patterns

A list of common problem-solving patterns for quant problems.

## Kelly Criterion

The optimal bet size (given as a proportion of your bankroll) for a series of bets is given by

$$
f = \frac{p}{a} - \frac{q}{b}
$$

where
* $p$ is the probability of a positive outcome
* $q$ is the probability of a negative outcome
* $a$ is the fraction of the bet lost in a negative outcome
* $b$ is the fraction of the bet won in a positive outcome

*Proof*: The proof is based on maximizing the geometric growth rate of wealth, given by

$$
r = (1 + fb)^p(1 - fa)^q
$$

Taking the log-derivative with respect to $f$ yields

$$
\frac{dr}{df} = \frac{pb}{1 + fb} - \frac{aq}{1 - fa}
$$

and setting to $0$ and solving for $f$ gives the desired result.

## Expected Number of Trials For First Success

The probability of needing $k$ trials before viewing the first success, with probability $p$,
is modelled by the geometric distribution

$$
\operatorname{Geom}(k; p) = (1 - p)^{k - 1} p
$$

which has expectation $\mathbb E[k] = \frac{1}{p}$.

## Expected Number of Independent Events

When solving problems that require counting the expected number of independent events that occur,
it can be useful to define indicator variables for each of the independent events, and using
the linearity of expectation to count the total.

**Example**: Let $S$ be a uniformly distributed randomly-generated 5-digit string.
What is the expected number of unique digits in $S$?

*Solution*: Define indicator variables $I_k$ where $I_k = 1$ if $k$ is present in $S$ and $0$
otherwise. Then the expected number of unique digits $X$ in $S$ is

$$
\mathbb E[X] = \mathbb E\left[\sum_{k = 0}^9 I_k\right] = \sum_{k = 0}^9 \mathbb E[I_k]
= 10\mathbb E[I_0]
$$

by linearity of expectation and symmetry of $I_0, \dots, I_9$. It remains to find $I_0$.
The probability of **not** observing $0$ in $S$ is $(9/10)^{5}$ (each digit is one of the 10 digits
other than 0). So the probability of observing at least one 0 in $S$ is $1 - (9/10)^{5}$. Thus

$$
\mathbb E[X] = 10\mathbb E[I_0] = 10 \cdot \left(1 - \left(\frac{9}{10}\right)^{5}\right) = 
\frac{40951}{10000}
$$

## Processes Whose Next State Depends Only on the Current State

**Definition**: A **Markov chain**

## Expected Time and Steps for Symmetric Random Walk

**Definition**: A **martingale** is a stochastic process (a sequence of random variables) $(X_n)$ with finite mean such that

$$
\mathbb E[X_n \mid X_1 = x_1, \dots, X_{n - 1} = x_{n - 1}] = x_{n - 1}
$$

That is, the expectation of any $X_n$ conditioned on all previous RVs is the value of the previous RV.

Intuitively, a martingale is a process whose "change" is expected to be 0 given the information of all the trials that happened before it. As a corollary, the expectation (not conditional) of the martingale is constant.

**Proposition**: If $(X_n)$ is a martingale, then

$$
\mathbb E[X_0] = \mathbb E[X_t]
$$

for all $t \geq 0$.

*Proof*: By the Law of Total Probability,

$$
\mathbb E[X_{t + 1}] =  \mathbb E[\mathbb E[X_{t + 1} \mid \mathcal F_t]] = \mathbb E[X_t]
$$

where $\mathcal F_t = \sigma(X_0, X_1, \dots, X_t)$ is the $\sigma$-algebra generated by $X_0, \dots, X_t$. The proposition then follows from induction $\blacksquare$

**Example**: Let $S_n$ be a random integer variable denoting the number of forward steps taken after $n$ steps, where each step has equal probability of being a forward step or a backward step. Then $(S_n)$ is a martingale since


$$
\begin{align*}
\mathbb E[S_n \mid S_1 = s_1, \dots, S_{n - 1} = s_{n - 1}]
&= S_{n - 1} + \frac{1}{2}(1) + \frac{1}{2}(-1) = S_{n - 1}
\end{align*}
$$

## Processes with Stopping Rules

**Definition**: Let $(X_n)$ be an IID stochastic process. A **stopping rule** for $(X_n)$ is a positive integer-valued random variable $N$ such that $P(N \leq n)$ is independent of $X_{n + 1}, X_{n + 2}, \dots$.

**Example**: Let $(X_n)$ be a stochastic process where $X_n = 1$ with probability $1/2$ and $0$ otherwise. The RV $N$ with distribution

$$
P(N = n) := P\left(\sum_{k = 0}^N X_k = 3\right)
$$

(i.e. the probability of $3$ heads in $n$ coin-flips) is a stopping rule since $P(N \leq n)$ depends only on $X_1, \dots, X_N$.

**Theorem (Wald's Equality)**: Let $N$ be a stopping rule for IID random variables $X_1, \dots, X_n$ and let $S_N = \sum_{k = 0}^N X_k$. Then

$$
\mathbb E[S_n] = \mathbb E[X]\mathbb E[N]
$$

**Example**: Suppose a game where you roll a dice until a $6$ appears. You are paid the face value for each roll. We find the expected value of the game, $\mathbb E[S_n]$ where $S_n = \sum_{k = 0}^n X_k$. 

The game stops when a 6 is rolled. Our stopping rule $N$ is then defined by

$$
P(N = n) = \left(\frac{5}{6}\right)^{n - 1}\left(\frac{1}{6}\right) = \operatorname{Geom}\left(\frac{1}{6}\right)
$$

The expected value of each roll is $\mathbb E[X] = 3.5$. So by Wald's Equality,

$$
\mathbb E[S_n] = \mathbb E[X]\mathbb E[N] = (3.5)(6) = 21
$$

## Expected Number of Trials Before Observing Sequence

**Theorem**: Let $X \sim \operatorname{Bernoulli}(p)$. The expected number of trials $\mathbb E[T]$ to view a specific sequence of failiures and successes is given by

$$
\mathbb E[T] = \sum_{k \in S} \left(\frac{1}{p}\right)^k
$$

where $S = \lbrace i : X_i \text{ in the sequence is a success} \rbrace$.

*Proof*: 

## Catalan Numbers

$$
\frac{1}{n + 1}\binom{2n}{n}
$$